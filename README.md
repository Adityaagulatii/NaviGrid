ğŸ—ºï¸ NaviGrid
Camera-based indoor navigation â€” no beacons, no 3D mapping, just your phone.

ğŸ¬ Watch NaviGrid in action:
[Demo](https://vimeo.com/1168262894?share=copy&fl=sv&fe=ci)

ğŸ“Œ Overview
NaviGrid simplifies indoor navigation by eliminating the need for expensive infrastructure. No BLE beacons. No Wi-Fi triangulation systems. No 3D building scans.
Users simply take a picture of a floorplan, annotate the rooms they want to navigate between, and NaviGrid handles the rest â€” using Computer Vision and OCR to understand the layout, determine the current location, calculate an optimal route, and deliver turn-by-turn directions.

Navigating any new indoor space is now as simple as snapping a photo.


ğŸ’¡ Inspiration
Indoor navigation has countless real-world applications â€” from helping new students find their way around campus, to enabling robotics and drone delivery systems, to improving accessibility for blind and visually impaired individuals.
Yet the current process is long, complex, and expensive. Traditional indoor navigation requires:

Deploying hardware like BLE beacons or Wi-Fi access points
Building full 3D models of the facility
All because GPS simply doesn't work indoors

We came across research tackling this exact problem:

Floorplan2Guide â€” uses LLMs to parse floorplans for blind and low-vision (BLV) navigation
Snap&Nav â€” a smartphone-based system that analyzes floor maps and detects intersections for indoor guidance

While impressive, both systems were still complex and resource-heavy. That's what inspired NaviGrid â€” we wanted to strip the entire indoor navigation process down to its simplest form.
No beacons. No 3D mapping. No expensive infrastructure. Just your camera.

âš™ï¸ How It Works
ğŸ“¸ Step 1 â€” Capture
        Take a photo of the building's floorplan

âœï¸ Step 2 â€” Annotate
        Mark the rooms or points you want to navigate between

ğŸ§  Step 3 â€” Parse
        NaviGrid processes the floorplan using Computer Vision and OCR

ğŸ•¸ï¸ Step 4 â€” Build Graph
        A routing graph is constructed from annotated nodes and edges

ğŸ§­ Step 5 â€” Navigate
        Turn-by-turn directions guide you to your destination

ğŸ¯ Use Cases

ğŸ“ Campus Navigation â€” Help new students find classrooms, offices, and facilities
â™¿ Accessibility â€” Empower blind and visually impaired individuals to navigate independently
ğŸ¤– Robotics â€” Provide autonomous systems with a lightweight indoor mapping solution
ğŸš Drone Delivery â€” Enable path planning for indoor drone navigation systems


ğŸ› ï¸ Tech Stack
ComponentTechnologyCore LogicPythonFloorplan ParsingComputer Vision (OpenCV) + OCR (Tesseract / EasyOCR)PathfindingGraph-based routing (NetworkX)Annotation Interface(specify your UI framework here)
